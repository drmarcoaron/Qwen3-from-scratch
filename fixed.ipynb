{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_5I2up39nnO"
      },
      "source": [
        " # Building Qwen3 from Scratch: A Complete\n",
        "\n",
        "\n",
        "\n",
        " Welcome,  we'll build a Qwen3-style language model from scratch!\n",
        "\n",
        " We'll implement Grouped-Query Attention (GQA), RMSNorm, SwiGLU activations, and add our own spin - new Muon optimizer that accelerates training by 30% to 50%.\n",
        "\n",
        "\n",
        "\n",
        " **What we'll cover:**\n",
        "\n",
        "- Modern Transformer architecture with Qwen3-style features\n",
        "\n",
        "- Grouped-Query Attention (GQA) for memory and compute efficiency\n",
        "\n",
        "- Rotary Positional Embeddings (RoPE) for better performance and context window extrapolation\n",
        "\n",
        "- QK-Norm with RMSNorm for improved numerical / training stability\n",
        "\n",
        "- Muon optimizer using Newton-Schulz orthogonalization for better weight updates, faster learning with less data\n",
        "\n",
        "- Hybrid optimization using Muon for matrices and AdamW for other parameters\n",
        "\n",
        "- SwiGLU activation and deep residual learning in the feedforward layers\n",
        "\n",
        "- Efficient dataset tokenization and caching with HuggingFace Datasets and Transformers\n",
        "\n",
        "- Validation metrics including loss, accuracy, and perplexity\n",
        "\n",
        "- Gradient accumulation + AMP (Automatic Mixed Precision) training for larger batch sizes\n",
        "\n",
        "- Cosine learning rate scheduling with warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8e5ds9P9nnS"
      },
      "source": [
        " ## 1. Setup and Imports\n",
        "\n",
        "\n",
        "\n",
        " First, let's import all the necessary libraries. We'll use PyTorch for the deep learning framework,\n",
        "\n",
        " transformers for tokenization, and various utilities for data handling and training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5htnrCxgeBC",
        "outputId": "588792cc-4029-48ca-9075-35b03963f9ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ybKLqFCux2mg",
        "outputId": "2e8f8622-4130-4a71-ef93-80cb959d271e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "collapsed": true,
        "id": "pjuQOPuPpHjX",
        "outputId": "5c6ae850-28c9-4e8e-8ec6-342b5fcad895"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e6d26d5b7074ad6abe7feb79909cccb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rQLuHlbT9nnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7843bb11-cde5-4d92-ebec-56ed3dfed840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup and imports Complete\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn  # Neural network modules like Linear, Embedding, etc.\n",
        "import torch.nn.functional as F  # Functional interface for operations like cross_entropy, silu, etc.\n",
        "from torch.utils.data import Dataset, DataLoader  # Base class and utilities for loading datasets\n",
        "from torch.cuda.amp import autocast, GradScaler  #  Automatic Mixed Precision (AMP) tools for faster/lower-memory training\n",
        "\n",
        "import math  # Standard math operations (e.g. sqrt, exp, cos)\n",
        "import random  # Python's random number utilities (used for seeding)\n",
        "import numpy as np  # Numerical computing library, used for random seeding and general array ops\n",
        "\n",
        "from datasets import load_dataset  #  Hugging Face Datasets library for streaming large datasets\n",
        "from tqdm import tqdm  #  Progress bar visualization library, great for loops\n",
        "\n",
        "import time  # \u231b Timing utilities, measuring time\n",
        "from transformers import AutoTokenizer  #  Load pretrained tokenizers from HuggingFace with one line\n",
        "\n",
        "from dataclasses import dataclass  #  Define simple classes for configs with less boilerplate\n",
        "from typing import List, Optional  #  Type hints for better readability and tooling\n",
        "\n",
        "import warnings  #  Suppress or handle warnings\n",
        "import os  #  File system operations (creating folders, path checking, etc.)\n",
        "import pickle  #  Python object serialization (used to save/load preprocessed datasets)\n",
        "\n",
        "warnings.filterwarnings('ignore')  # Silences warnings for cleaner outputs during training\n",
        "\n",
        "print(\"Setup and imports Complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8LriEQ9nnU"
      },
      "source": [
        " ## 2. Utility Functions\n",
        "\n",
        "\n",
        "\n",
        " Let's start with some utility functions for reproducibility and configuration management.\n",
        "\n",
        " The `set_seed` function ensures our experiments are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3CczY5ms9nnU"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\" Set all seeds to {seed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-5evpnH9nnU"
      },
      "source": [
        " ## 3. Model Configuration\n",
        "\n",
        "\n",
        "\n",
        " Here we define our model configuration using a dataclass. This makes it easy to experiment\n",
        "\n",
        " with different model sizes and hyperparameters. Our model will be a smaller version of Qwen3\n",
        "\n",
        " with 384 dimensions, 6 layers, and 8 attention heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Buy5jS939nnV"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    # Model architecture\n",
        "    d_model: int = 384\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 6\n",
        "    d_ff: int = 1536\n",
        "    batch_size: int = 24\n",
        "    max_steps: int = 2000\n",
        "\n",
        "    '''To get number of parameters of our model simply = 12 * n_layers * (d_model) * (d_model)'''\n",
        "\n",
        "    # Qwen3-like parameters\n",
        "    n_kv_heads: int = 4  # For Grouped-Query Attention\n",
        "    sliding_window: int = 4096  # Set a large default, effectively disabling it unless specified\n",
        "    attention_bias: bool = False  # Qwen3 often sets this to False\n",
        "    rms_norm_eps: float = 1e-6  # Epsilon for RMSNorm\n",
        "\n",
        "    # Training parameters\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    muon_lr: float = 0.01\n",
        "\n",
        "    # Data parameters\n",
        "    max_seq_len: int = 512\n",
        "    num_documents: int = 2000\n",
        "    max_tokens: int = 500000\n",
        "\n",
        "    # Evaluation\n",
        "    eval_every: int = 500\n",
        "    eval_steps: int = 100\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay: float = 0.1\n",
        "    dropout: float = 0.1\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # Technical\n",
        "    use_amp: bool = True\n",
        "    vocab_size: Optional[int] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_k = self.d_model // self.n_heads\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_groups = self.n_heads // self.n_kv_heads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7rRp5Ac9nnV"
      },
      "source": [
        " ## 4. Grouped-Query Attention Helper\n",
        "\n",
        "\n",
        "\n",
        " This function implements the key component of GQA - repeating key and value heads.\n",
        "\n",
        " In GQA, we have fewer key-value heads than query heads, which reduces memory usage\n",
        "\n",
        " while maintaining performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yucDeNey9nnV"
      },
      "outputs": [],
      "source": [
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n",
        "    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim)\n",
        "    to (batch, num_attention_heads, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    # Extract dimensions from input tensor\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "\n",
        "    # Early return if no repetition is needed\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "\n",
        "    # Add a new dimension at index 2 (after num_key_value_heads) and expand\n",
        "    # Shape transformation:\n",
        "    # (batch, num_key_value_heads, slen, head_dim)\n",
        "    # -> (batch, num_key_value_heads, 1, slen, head_dim) [via None indexing]\n",
        "    # -> (batch, num_key_value_heads, n_rep, slen, head_dim) [via expand]\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "\n",
        "    # Flatten the num_key_value_heads and n_rep dimensions together\n",
        "    # Final shape: (batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "    # This effectively repeats each key/value head n_rep times\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHoK7wtV9nnV"
      },
      "source": [
        " ## 5. Muon Optimizer - The Secret Sauce\n",
        "\n",
        "\n",
        "\n",
        " The Muon optimizer is a novel approach that uses Newton-Schulz iteration for orthogonalization.\n",
        "\n",
        " This helps with training stability and convergence. The `zeropower_via_newtonschulz5` function\n",
        "\n",
        " implements the core mathematical operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P7jXXxGz9nnW"
      },
      "outputs": [],
      "source": [
        "@torch.compile\n",
        "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
        "    \"\"\"Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\"\"\"\n",
        "    assert G.ndim >= 2\n",
        "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
        "    X = G.bfloat16()\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.mT\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    return X\n",
        "\n",
        "class Muon(torch.optim.Optimizer):\n",
        "    \"\"\"Muon - MomentUm Orthogonalized by Newton-schulz\"\"\"\n",
        "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
        "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                g = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # Initialize momentum buffer if first time\n",
        "                if \"momentum_buffer\" not in state:\n",
        "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
        "\n",
        "                buf = state[\"momentum_buffer\"]\n",
        "                # Update momentum buffer: buf = momentum * buf + (1-momentum) * grad\n",
        "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
        "                # Apply Nesterov momentum if enabled, otherwise use standard momentum\n",
        "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
        "                # Apply zero-power normalization via Newton-Schulz iterations (make it close to orthonormal)\n",
        "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
        "                # Update parameters with adaptive scaling based on parameter shape\n",
        "                p.add_(g.view_as(p), alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n",
        "                # Updates parameters with an adaptive learning rate that scales based on the parameter tensor's aspect ratio (height/width). For matrices where height > width, it increases the effective learning rate by \u221a(height/width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0RdGidT9nnW"
      },
      "source": [
        " ## 6. Data Loading and Caching\n",
        "\n",
        "\n",
        "\n",
        " Loading and processing data can be time-consuming. We implement caching to avoid\n",
        "\n",
        " reprocessing the same data multiple times. This function loads the SmolLM corpus\n",
        "\n",
        " and tokenizes it efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ql6arQ7J9nnW"
      },
      "outputs": [],
      "source": [
        "def load_and_cache_data(config: ModelConfig, cache_dir: str = \"data_cache\"):\n",
        "    \"\"\"Load and cache tokenized data to avoid reprocessing\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_file = f\"{cache_dir}/tokenized_data_{config.num_documents}_{config.max_tokens}.pkl\"\n",
        "\n",
        "    # Check if cached data exists\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"\ud83d\udce6 Loading cached data from {cache_file}\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            cached_data = pickle.load(f)\n",
        "\n",
        "        texts = cached_data['texts']\n",
        "        tokenizer = cached_data['tokenizer']\n",
        "        tokens = cached_data['tokens']\n",
        "        config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "        print(f\"\u2705 Loaded {len(texts)} documents, {len(tokens):,} tokens from cache\")\n",
        "        return texts, tokenizer, tokens\n",
        "\n",
        "    print(f\"\ud83d\udd04 Processing new data (will cache for future use)\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n",
        "\n",
        "    texts = []\n",
        "    for i, item in enumerate(dataset):\n",
        "        if i >= config.num_documents:\n",
        "            break\n",
        "        texts.append(item[\"text\"][:3000])\n",
        "\n",
        "    print(f\"Loaded {len(texts)} documents\")\n",
        "\n",
        "    # Tokenize\n",
        "    print(\"Tokenizing texts...\")\n",
        "    all_tokens = []\n",
        "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    tokens = all_tokens[:config.max_tokens]\n",
        "    print(f\"Using {len(tokens):,} tokens\")\n",
        "    config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Cache the processed data\n",
        "    cached_data = {'texts': texts, 'tokenizer': tokenizer, 'tokens': tokens}\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(cached_data, f)\n",
        "\n",
        "    print(f\"\ud83d\udcbe Cached data to {cache_file}\")\n",
        "    return texts, tokenizer, tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBsAbVgW9nnW"
      },
      "source": [
        " ## 7. Dataset Class\n",
        "\n",
        "\n",
        "\n",
        " We create a custom dataset class for language modeling. This creates sliding windows\n",
        "\n",
        " of tokens for training, where each sample is a sequence and its corresponding target\n",
        "\n",
        " (shifted by one position)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ccgG3UnR9nnW"
      },
      "outputs": [],
      "source": [
        "class TextTokenDataset(Dataset):\n",
        "    def __init__(self, tokens: List[int], seq_len: int = 512):\n",
        "        self.tokens = tokens\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.tokens) - self.seq_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hXTnH-z9nnX"
      },
      "source": [
        " ## 8. Rotary Position Embeddings (RoPE)\n",
        "\n",
        "\n",
        "\n",
        " RoPE is a modern alternative to positional encodings that allows the model to\n",
        "\n",
        " generalize to longer sequences. It applies rotation matrices to the embeddings\n",
        "\n",
        " based on their position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WuuNzqYb9nnX"
      },
      "outputs": [],
      "source": [
        "class Rotary(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        angular_freq = (1 / 10000) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)\n",
        "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])\n",
        "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
        "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
        "        self.register_buffer('cos', theta.cos(), persistent=False)\n",
        "        self.register_buffer('sin', theta.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, x_BTHD: torch.Tensor):\n",
        "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
        "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
        "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
        "        y1 = x1 * cos + x2 * sin\n",
        "        y2 = x1 * (-sin) + x2 * cos\n",
        "        return torch.cat((y1, y2), 3).type_as(x_BTHD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCJaNEH49nnX"
      },
      "source": [
        " ## 9. Grouped-Query Attention Implementation\n",
        "\n",
        "\n",
        "\n",
        " This is the heart of our model - the attention mechanism with GQA. Notice how we:\n",
        "\n",
        " 1. Project Q, K, V separately\n",
        "\n",
        " 2. Apply QK normalization (a Qwen3 innovation)\n",
        "\n",
        " 3. Use RoPE for positional information\n",
        "\n",
        " 4. Implement GQA by repeating K and V heads\n",
        "\n",
        " 5. Use scaled dot-product attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7gASpg2l9nnX"
      },
      "outputs": [],
      "source": [
        "class Qwen3Attention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.n_heads = config.n_heads\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_kv_groups = config.n_kv_groups\n",
        "        self.d_k = config.d_k\n",
        "\n",
        "        # Separate linear layers for Q, K, V\n",
        "        self.q_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "        # QK-Normalization layers\n",
        "        self.q_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "        self.k_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "\n",
        "        self.rotary = Rotary(self.d_k, config.max_seq_len)\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # 1. Project Q, K, V separately\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # 2. Reshape into heads\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "        k = k.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "        v = v.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "\n",
        "        # 3. Apply QK-Norm\n",
        "        q = self.q_norm(q)\n",
        "        k = self.k_norm(k)\n",
        "\n",
        "        # 4. Apply RoPE\n",
        "        # Transpose to (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k) for rotary\n",
        "        q = self.rotary(q.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "        k = self.rotary(k.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Transpose for attention: (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
        "        Q = q.transpose(1, 2)\n",
        "        K = k.transpose(1, 2)\n",
        "        V = v.transpose(1, 2)\n",
        "\n",
        "        # 5. Repeat K and V heads for GQA\n",
        "        K = repeat_kv(K, self.n_kv_groups)\n",
        "        V = repeat_kv(V, self.n_kv_groups)\n",
        "\n",
        "        # 6. Scaled Dot-Product Attention\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V, is_causal=True, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "\n",
        "        # 7. Reshape and final projection\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.w_o(attn_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKYp-oKf9nnX"
      },
      "source": [
        " ## 10. SwiGLU Feed-Forward Network\n",
        "\n",
        "\n",
        "\n",
        " SwiGLU is a modern activation function that combines Swish and GLU. It's more\n",
        "\n",
        " effective than traditional ReLU and is used in many modern models including Qwen3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "05g5swnW9nnX"
      },
      "outputs": [],
      "source": [
        "class SwiGLUFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implementation of the SwiGLU activation function\n",
        "        # F.silu is the Swish activation function\n",
        "        activated_x = F.silu(self.gate_proj(x)) * self.up_proj(x)\n",
        "        return self.down_proj(self.dropout(activated_x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think of:\n",
        "\n",
        "`output = gate(x) * value(x)`\n",
        "\n",
        "like:\n",
        "\n",
        "`light = brightness_control \u00d7 light_source`\n"
      ],
      "metadata": {
        "id": "Fut7QDnDpT1H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ0YpmSk9nnX"
      },
      "source": [
        " ## 11. Transformer Block\n",
        "\n",
        "\n",
        "\n",
        " Each transformer block combines attention and feed-forward layers with residual\n",
        "\n",
        " connections and normalization. We use RMSNorm instead of LayerNorm for better\n",
        "\n",
        " training stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dxaNJjGQ9nnX"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):  # Pass the entire config object\n",
        "        super().__init__()\n",
        "        self.attention = Qwen3Attention(config)\n",
        "        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n",
        "        self.norm1 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.norm2 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attention(self.norm1(x))\n",
        "        x = x + self.dropout(attn_out)\n",
        "        ff_out = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg9Sk2mV9nnX"
      },
      "source": [
        " ## 12. Complete Language Model\n",
        "\n",
        "\n",
        "\n",
        " Now we assemble everything into our complete language model. This includes:\n",
        "\n",
        " - Token embeddings\n",
        "\n",
        " - Positional dropout\n",
        "\n",
        " - Stack of transformer blocks\n",
        "\n",
        " - Final normalization and output projection\n",
        "\n",
        " - Weight tying between input embeddings and output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TacWbKLz9nnX"
      },
      "outputs": [],
      "source": [
        "class MinimalLLM(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.output_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Tie weights\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_embedding.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
        "        x = self.position_dropout(x)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.output_dropout(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w5nsjPZ9nnX"
      },
      "source": [
        " ## 13. Evaluation Function\n",
        "\n",
        "\n",
        "\n",
        " During training, we need to evaluate our model's performance. This function\n",
        "\n",
        " computes loss, accuracy, and perplexity on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p6Ov0uO29nnY"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: ModelConfig):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation (saves memory and computation)\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            # Stop evaluation after specified number of steps to limit eval time\n",
        "            if i >= config.eval_steps:\n",
        "                break\n",
        "\n",
        "            # Move input sequences (x) and target sequences (y) to GPU/device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Use automatic mixed precision if enabled (faster training with minimal accuracy loss)\n",
        "            with autocast(enabled=config.use_amp):\n",
        "                # Forward pass: get model predictions (logits) for input sequence\n",
        "                logits = model(x)\n",
        "\n",
        "                # Calculate cross-entropy loss between predictions and targets\n",
        "                # Reshape to (batch_size * seq_len, vocab_size) and (batch_size * seq_len,)\n",
        "                # for proper cross-entropy computation across all token positions\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "            # Accumulate total loss weighted by number of tokens in this batch\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            # Keep track of total number of tokens processed\n",
        "            total_tokens += y.numel()\n",
        "\n",
        "            # Get predicted token IDs by taking argmax over vocabulary dimension\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            # Count correct predictions for accuracy calculation\n",
        "            total_correct += (predictions == y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    accuracy = total_correct / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 20))\n",
        "\n",
        "    model.train()\n",
        "    return {'val_loss': avg_loss, 'val_accuracy': accuracy, 'val_perplexity': perplexity}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qjZRIpd9nnY"
      },
      "source": [
        " ## 14. Optimizer Setup\n",
        "\n",
        "\n",
        "\n",
        " We use a hybrid approach: Muon optimizer for 2D parameters (attention and feed-forward weights)\n",
        "\n",
        " and AdamW for other parameters. This gives us the benefits of both optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o3gfmjW29nnY"
      },
      "outputs": [],
      "source": [
        "def setup_muon_optimizer(model: nn.Module, config: ModelConfig):\n",
        "    \"\"\"Setup Muon optimizer with hybrid approach\"\"\"\n",
        "    muon_params = []\n",
        "    adamw_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if (param.ndim == 2 and\n",
        "            'token_embedding' not in name and\n",
        "            'norm' not in name and\n",
        "            param.requires_grad):\n",
        "            muon_params.append(param)\n",
        "        else:\n",
        "            adamw_params.append(param)\n",
        "\n",
        "    print(f\"  Muon parameters: {sum(p.numel() for p in muon_params):,}\")\n",
        "    print(f\"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}\")\n",
        "\n",
        "    muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=0.95)\n",
        "    adamw_optimizer = torch.optim.AdamW(adamw_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay)\n",
        "\n",
        "    return [muon_optimizer, adamw_optimizer]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y_5AhJM9nnY"
      },
      "source": [
        " ## 15. Training Loop\n",
        "\n",
        "\n",
        "\n",
        " This is where the magic happens! Our training loop includes:\n",
        "\n",
        " - Gradient accumulation for larger effective batch sizes\n",
        "\n",
        " - Mixed precision training for speed\n",
        "\n",
        " - Learning rate scheduling with warmup and cosine decay\n",
        "\n",
        " - Regular evaluation and model checkpointing\n",
        "\n",
        " - Progress tracking with detailed metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RT4qQBd_9nnY"
      },
      "outputs": [],
      "source": [
        "def train_model(config: ModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n",
        "    \"\"\"Train the model with Muon optimizer\"\"\"\n",
        "    print(f\"\\n\ud83d\ude80 Training Small model with Muon optimizer\")\n",
        "\n",
        "    # Initialize model\n",
        "    set_seed(42)\n",
        "    model = MinimalLLM(config)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  \ud83d\udcca Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Setup optimizers\n",
        "    optimizers = setup_muon_optimizer(model, config)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    schedulers = []\n",
        "    for optimizer in optimizers:\n",
        "        warmup_steps = config.max_steps // 20\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)\n",
        "                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "        schedulers.append(scheduler)\n",
        "\n",
        "    scaler = GradScaler() if config.use_amp else None\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    step = 0\n",
        "    start_time = time.time()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    pbar = tqdm(total=config.max_steps, desc=\"Training\")\n",
        "\n",
        "    while step < config.max_steps:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            if step >= config.max_steps:\n",
        "                break\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass with gradient accumulation\n",
        "            if config.use_amp:\n",
        "                with autocast():\n",
        "                    logits = model(x)\n",
        "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                    loss = loss / config.gradient_accumulation_steps\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimizer step after accumulation\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                if config.use_amp:\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.step(optimizer)\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "\n",
        "            # Logging\n",
        "            if step % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    predictions = logits.argmax(dim=-1)\n",
        "                    accuracy = (predictions == y).float().mean().item()\n",
        "                    current_loss = loss.item() * config.gradient_accumulation_steps\n",
        "                    perplexity = math.exp(min(current_loss, 20))\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{current_loss:.4f}',\n",
        "                    'acc': f'{accuracy:.3f}',\n",
        "                    'ppl': f'{perplexity:.1f}',\n",
        "                    'lr': f'{optimizers[0].param_groups[0][\"lr\"]:.2e}'\n",
        "                })\n",
        "\n",
        "            # Evaluation\n",
        "            if step % config.eval_every == 0 and step > 0:\n",
        "                eval_metrics = evaluate_model(model, val_loader, config)\n",
        "                print(f\"\\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, \"\n",
        "                      f\"Val Acc: {eval_metrics['val_accuracy']:.4f}, \"\n",
        "                      f\"Val PPL: {eval_metrics['val_perplexity']:.2f}\")\n",
        "\n",
        "                if eval_metrics['val_loss'] < best_val_loss:\n",
        "                    best_val_loss = eval_metrics['val_loss']\n",
        "                    # Save best model\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'config': config,\n",
        "                        'step': step,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'final_metrics': eval_metrics\n",
        "                    }, 'best_model.pt')\n",
        "                    print(f\"\ud83d\udcbe Saved best model with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            step += 1\n",
        "            if step % 10 == 0:\n",
        "                pbar.update(10)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"  \u23f1\ufe0f Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "    # Final evaluation\n",
        "    final_eval = evaluate_model(model, val_loader, config)\n",
        "    print(f\"  \ud83d\udcca Final - Loss: {final_eval['val_loss']:.4f}, \"\n",
        "          f\"Acc: {final_eval['val_accuracy']:.4f}, PPL: {final_eval['val_perplexity']:.2f}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'step': step,\n",
        "        'final_metrics': final_eval\n",
        "    }, 'final_model.pt')\n",
        "    print(f\"\ud83d\udcbe Saved final model to final_model.pt\")\n",
        "\n",
        "    return model, final_eval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpRsvalr9nnY"
      },
      "source": [
        " ## 16. Main Training Script\n",
        "\n",
        "\n",
        "\n",
        " Finally, let's put everything together! This section:\n",
        "\n",
        " 1. Checks system resources\n",
        "\n",
        " 2. Sets up configuration\n",
        "\n",
        " 3. Loads and prepares data\n",
        "\n",
        " 4. Trains the model\n",
        "\n",
        " 5. Reports final results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2ufOMN_19nnY",
        "outputId": "e6e9c86a-0560-427d-999f-104e503a580c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udd0d Device: CUDA\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n",
            " Set all seeds to 42\n",
            "\n",
            "\ud83d\udccb Model Configuration:\n",
            "   Architecture: 384d, 6L, 8H, 1536ff\n",
            "   Training: 2000 steps, batch size 24\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "\ud83d\udd04 Processing new data (will cache for future use)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33739fab6cf8441a93a1c50007c822bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3e1eb409bf14156b83b538e06965c5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24c549821e8c413a8bbe8c7ae952b5ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "baf0ed662a4c44fa85dd0a49eebef8ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d666d0b4ccb8445abd0d0c19e9aa03b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d442306351a348a8a20e48d430511311"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af1b91e16bd34abeb3e6bd005a9a35ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abc88f825e8c49b7b4588753870cc2f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2000 documents\n",
            "Tokenizing texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:03<00:00, 664.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 500,000 tokens\n",
            "\ud83d\udcbe Cached data to data_cache/tokenized_data_2000_500000.pkl\n",
            "\ud83d\udcca Dataset: 449540 train, 49948 val samples\n",
            "\n",
            "\ud83d\ude80 Training Small model with Muon optimizer\n",
            " Set all seeds to 42\n",
            "  \ud83d\udcca Total parameters: 32,150,976\n",
            "  Muon parameters: 13,271,040\n",
            "  AdamW parameters: 18,879,936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/2000 [00:01<?, ?it/s, loss=10.8037, acc=0.015, ppl=49202.1, lr=0.00e+00]W0928 04:49:17.365000 1648 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Training:  25%|\u2588\u2588\u258c       | 500/2000 [02:50<08:33,  2.92it/s, loss=5.2131, acc=0.223, ppl=183.7, lr=1.00e-02]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 500: Val Loss: 4.9945, Val Acc: 0.2327, Val PPL: 147.59\n",
            "\ud83d\udcbe Saved best model with val_loss: 4.9945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|\u2588\u2588\u2588\u2588\u2588     | 1000/2000 [05:54<05:49,  2.86it/s, loss=3.5962, acc=0.358, ppl=36.5, lr=9.86e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1000: Val Loss: 3.1809, Val Acc: 0.4079, Val PPL: 24.07\n",
            "\ud83d\udcbe Saved best model with val_loss: 3.1809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 1500/2000 [08:59<02:54,  2.86it/s, loss=2.5042, acc=0.495, ppl=12.2, lr=9.54e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1500: Val Loss: 1.9007, Val Acc: 0.6069, Val PPL: 6.69\n",
            "\ud83d\udcbe Saved best model with val_loss: 1.9007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [12:04<00:00,  2.76it/s, loss=1.8120, acc=0.603, ppl=6.1, lr=9.06e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u23f1\ufe0f Training completed in 724.6 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \ud83d\udcca Final - Loss: 1.1133, Acc: 0.7514, PPL: 3.04\n",
            "\ud83d\udcbe Saved final model to final_model.pt\n",
            "\n",
            "\ud83c\udf89 TRAINING COMPLETED!\n",
            "\u23f1\ufe0f Total time: 12.3 minutes\n",
            "\ud83c\udfc6 Final Results:\n",
            "   Validation Loss: 1.1133\n",
            "   Validation Accuracy: 0.7514\n",
            "   Validation Perplexity: 3.04\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check system\n",
        "    print(f\"\ud83d\udd0d Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(42)\n",
        "\n",
        "    # Create config for Small model\n",
        "    config = ModelConfig()\n",
        "    print(f\"\\n\ud83d\udccb Model Configuration:\")\n",
        "    print(f\"   Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "    print(f\"   Training: {config.max_steps} steps, batch size {config.batch_size}\")\n",
        "    print(f\"   Data: {config.max_tokens:,} tokens, seq_len {config.max_seq_len}\")\n",
        "\n",
        "    # Load data\n",
        "    texts, tokenizer, tokens = load_and_cache_data(config)\n",
        "    dataset = TextTokenDataset(tokens, config.max_seq_len)\n",
        "\n",
        "    # Train/val split\n",
        "    val_size = len(dataset) // 10\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"\ud83d\udcca Dataset: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    model, final_metrics = train_model(config, train_loader, val_loader)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n\ud83c\udf89 TRAINING COMPLETED!\")\n",
        "    print(f\"\u23f1\ufe0f Total time: {total_time/60:.1f} minutes\")\n",
        "    print(f\"\ud83c\udfc6 Final Results:\")\n",
        "    print(f\"   Validation Loss: {final_metrics['val_loss']:.4f}\")\n",
        "    print(f\"   Validation Accuracy: {final_metrics['val_accuracy']:.4f}\")\n",
        "    print(f\"   Validation Perplexity: {final_metrics['val_perplexity']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17. Model Loading and Inference\n",
        "\n",
        "After training, we can load our saved model and use it for text generation.\n",
        "This section shows how to load the trained model and perform inference."
      ],
      "metadata": {
        "id": "mcHNleswBtA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_trained_model(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Load a trained model from checkpoint\"\"\"\n",
        "    print(f\" Loading model from {model_path}\")\n",
        "\n",
        "    # Add ModelConfig to safe globals for PyTorch 2.6+\n",
        "    from torch.serialization import add_safe_globals\n",
        "    add_safe_globals([ModelConfig])\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location='cpu')\n",
        "        config = checkpoint['config']\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Error loading with weights_only=True, trying with weights_only=False...\")\n",
        "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
        "        config = checkpoint['config']\n",
        "\n",
        "    # Create model with same config\n",
        "    model = MinimalLLM(config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\u2705 Model loaded successfully\")\n",
        "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    return model, config"
      ],
      "metadata": {
        "id": "jkJxa1P6Bz2e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \ud83d\udd2e Text Generation Function\n",
        "\n",
        "This function generates text using a trained language model. Given a prompt, it tokenizes the input and autoregressively samples tokens up to `max_length`. It supports:\n",
        "\n",
        "- **Temperature scaling** for randomness control, e.g., `0.7` makes output more focused, `1.5` makes it more random.\n",
        "- **Top-k sampling** to limit candidates to the top `k` most likely tokens, e.g., `top_k=50` narrow down to 50 highest-probability tokens.\n",
        "- **Top-p (nucleus) sampling** to sample from the smallest set of tokens whose cumulative probability exceeds `p`, i.e. the fewest number of tokens whose combined probabilities add up to at least p (e.g., 90%).\n",
        "\n",
        "Generation stops early if the EOS token is produced.\n"
      ],
      "metadata": {
        "id": "UoFBxnhXCNPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model: nn.Module, tokenizer, prompt: str, max_length: int = 100,\n",
        "                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
        "\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get model predictions\n",
        "            logits = model(generated_ids)\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0:\n",
        "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
        "                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
        "                next_token_logits[top_k_indices] = top_k_logits\n",
        "\n",
        "            # Apply top-p (nucleus) filtering\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "                sorted_indices_to_remove[0] = 0\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample next token\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to generated sequence - FIX: ensure same dimensions\n",
        "            next_token = next_token.unsqueeze(0)  # Add batch dimension\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "            # Stop if we reach the end token\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "SPJIgtbIB0_W"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_inference(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Interactive inference session\"\"\"\n",
        "    print(\"\ud83e\udd16 Starting interactive inference session\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, config = load_trained_model(model_path)\n",
        "\n",
        "    # Load tokenizer (assuming we have the same one used during training)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            prompt = input(\"\\n Enter your prompt: \")\n",
        "            if prompt.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"\ud83d\udc4b Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not prompt.strip():\n",
        "                continue\n",
        "\n",
        "            print(\"\ud83d\udd04 Generating...\")\n",
        "            generated_text = generate_text(\n",
        "                model, tokenizer, prompt,\n",
        "                max_length=150,\n",
        "                temperature=0.8,\n",
        "                top_k=50,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            print(f\"\\n Generated text:\")\n",
        "            print(f\"\ud83d\udcdd {generated_text}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\ud83d\udc4b Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Error: {e}\")"
      ],
      "metadata": {
        "id": "TIxiUUKbB3F2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_inference(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Run a quick demo of the model's capabilities\"\"\"\n",
        "    print(\"\ud83c\udfad Running inference demo\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, config = load_trained_model(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Demo prompts\n",
        "    demo_prompts = [\n",
        "        \"Mama anakuja nyumbani Mwanza\",\n",
        "        \"Once upon a time in a distant galaxy\",\n",
        "        \"Tanzania ilipata uhuru mwaka\",\n",
        "        \"In the year 2050, technology will\",\n",
        "        \"The best way to learn programming is\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(demo_prompts, 1):\n",
        "        print(f\"\\n Demo {i}: '{prompt}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        generated_text = generate_text(\n",
        "            model, tokenizer, prompt,\n",
        "            max_length=100,\n",
        "            temperature=0.7,\n",
        "            top_k=40,\n",
        "            top_p=0.85\n",
        "        )\n",
        "\n",
        "        print(f\"\ud83d\udcdd {generated_text}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "F57LSWDkB44U"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if we have a trained model\n",
        "    import os\n",
        "\n",
        "    if os.path.exists(\"final_model.pt\"):\n",
        "        print(\"\ud83c\udf89 Found trained model! Running demo...\")\n",
        "        demo_inference(\"final_model.pt\")\n",
        "\n",
        "        # Optionally run interactive session\n",
        "        response = input(\"\\n\ud83e\udd16 Would you like to try interactive inference? (y/n): \")\n",
        "        if response.lower() in ['y', 'yes']:\n",
        "            interactive_inference(\"final_model.pt\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f No trained model found. Please run the training cells first.\")\n",
        "        print(\"\ud83d\udca1 Look for 'final_model.pt' or 'best_model.pt' in your directory.\")"
      ],
      "metadata": {
        "id": "oTTx359KB6W_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b52f4d7f-1e5f-4438-9934-4947428261bf",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udf89 Found trained model! Running demo...\n",
            "\ud83c\udfad Running inference demo\n",
            " Loading model from final_model.pt\n",
            "\u2705 Model loaded successfully\n",
            "   Parameters: 32,150,976\n",
            "   Device: cuda\n",
            "\n",
            " Demo 1: 'Mama anakuja nyumbani Mwanza'\n",
            "--------------------------------------------------\n",
            "\ud83d\udcdd Mama anakuja nyumbani Mwanza, 17, a few iconic Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese, and social structures caused by various social structures. This is the US, and other African cultures worldwide. Imagine being able to honor while still visible today, and politics, and other cultures moved to honor while exploring these concepts being able to change on how they are living on whether they occur on a few people of the US\n",
            "\n",
            "\n",
            " Demo 2: 'Once upon a time in a distant galaxy'\n",
            "--------------------------------------------------\n",
            "\ud83d\udcdd Once upon a time in a distant galaxy area where things can do!\n",
            "But why does this matter? By understanding human behavior, we can learn to recognize when we understand the power of their lives behind the power of life and appreciate the power of prayer.\n",
            "So, let's dive into the world of existence of forensic anthropology, and even risking their experiences of the importance of their experiences of human.\n",
            "Chapter 6: The Journey of the World of paper is a short-5: Exploring the World of a short-50%\n",
            "\n",
            "\n",
            " Demo 3: 'Tanzania ilipata uhuru mwaka'\n",
            "--------------------------------------------------\n",
            "\ud83d\udcdd Tanzania ilipata uhuru mwaka (acne\n",
            "#### Tidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesidesides\n",
            "\n",
            "\n",
            " Demo 4: 'In the year 2050, technology will'\n",
            "--------------------------------------------------\n",
            "\ud83d\udcdd In the year 2050, technology will be able to create beautiful and a way of a unique cultural significance. In this chapter, we will explore the fascinating world of juvenile fiction activity books, specifically focusing on the subgenre of juvenile fiction activity books, specifically written written written works that contain key books, and interactive elements of juvenile fiction activity books, and historical fiction activity books, and critical thinking, and juvenile fiction activity books. Through an examination of juvenile fiction activity books, particularly within the subgenre. Through a comprehensive guide will delve into the narrative while\n",
            "\n",
            "\n",
            " Demo 5: 'The best way to learn programming is'\n",
            "--------------------------------------------------\n",
            "\ud83d\udcdd The best way to learn programming is working with each other using Python. We'll need to create a popular library for Python using the following command in Python code snippet:\n",
            "```python\n",
            "import numpy as np\n",
            "def summary(A, `@app = np.array(\n",
            "    return np.array(A, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0\n",
            "\n",
            "\n",
            "\ud83e\udd16 Would you like to try interactive inference? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "XQKQtq8I1ab7",
        "outputId": "2747fb27-2037-436b-f5b1-0c8093eb3d5d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0220ba84d83d4a0da5824c9ad73381cc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "# Create repo\n",
        "create_repo(\"marcoharuni95/qwen3-small-muon\", exist_ok=True)\n",
        "\n",
        "# Push model\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"final_model.pt\",\n",
        "    path_in_repo=\"final_model.pt\",\n",
        "    repo_id=\"marcoharuni95/qwen3-small-muon\",\n",
        ")\n",
        "print(\"Model pushed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "collapsed": true,
        "id": "s0lhMlve1MCL",
        "outputId": "a1c2896a-efb3-4d9d-f81e-1d1f5e82131a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fbbfa348957422682a42b79d2607a4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8289e798d80a4f3b9005040fa8dc76c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  final_model.pt              :   0%|          |  561kB /  129MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67e4b555f2d74e4480d3af982d562549"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model pushed successfully\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ZH1RhXtTSyvG"
      ]
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}